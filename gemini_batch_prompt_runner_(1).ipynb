{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpupkies/Jim-Pupkies/blob/master/gemini_batch_prompt_runner_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtnlGahUdBLW",
        "outputId": "19a19fa3-7fa3-4000-b8ab-7c274a0b87ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Make a local screenshots folder\n",
        "os.makedirs('screenshots', exist_ok=True)\n",
        "\n",
        "# Copy files from Drive to Colab\n",
        "src = '/content/drive/MyDrive/AI_Portfolio/screenshots/'\n",
        "dst = 'screenshots/'\n",
        "shutil.copytree(src, dst, dirs_exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V_s8f6CsdM5i",
        "outputId": "deea9c78-0286-4616-a002-e4307b198ecb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'screenshots/'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini Batch Prompt Runner â€“ Portfolio Project\n",
        "\n",
        "This project demonstrates a repeatable AI batch-processing workflow using the Google Gemini API in Python.  \n",
        "It includes:\n",
        "- Prompt management via CSV files\n",
        "- Automated response generation using Gemini models\n",
        "- Error handling and clean output saving\n",
        "- Workflow visualization and documentation\n",
        "- Screenshots of execution and results\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "1. Project Overview\n",
        "2. Section 1 â€“ Setup & Secure API Key\n",
        "3. Step 1 â€” Create & Load Prompts\n",
        "4. Section 2 â€“ Batch Workflow Function\n",
        "5. Step 3 â€” Run Batch Process & Save Outputs\n",
        "6. Step 4 â€” Workflow Diagram\n",
        "7. Screenshots / Evidence of Execution\n",
        "8. Skills Demonstrated & Next Steps\n",
        "\n",
        "---\n",
        "\n",
        "> ðŸ”Ž **Start scrolling below to view each section, code, and output.**\n"
      ],
      "metadata": {
        "id": "4q-ofxY_XgGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1 â€“ Setup & Secure API Key\n",
        "This cell imports all necessary libraries, securely prompts for your Gemini API key at runtime, and initializes the Google Gemini client. It also optionally lists available Gemini models.\n"
      ],
      "metadata": {
        "id": "sWXsfSCzayya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 1: Setup\n",
        "import getpass\n",
        "import os\n",
        "import pandas as pd\n",
        "from google import genai\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Securely input your API key at runtime\n",
        "os.environ[\"GENAI_API_KEY\"] = getpass.getpass(\"Enter your Gemini API key: \")\n",
        "\n",
        "# Initialize client\n",
        "client = genai.Client(api_key=os.getenv(\"GENAI_API_KEY\"))\n",
        "\n",
        "# List available models (optional)\n",
        "print(\"Available Gemini models:\")\n",
        "for m in client.models.list():\n",
        "    print(m.name)\n"
      ],
      "metadata": {
        "id": "0kYGMSs8a1l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "print(platform.python_version())\n",
        "print(platform.architecture())\n"
      ],
      "metadata": {
        "id": "akdwVdww6xys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "id": "ehAXqPOLKjvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Name of your project folder (generic for portfolio)\n",
        "repo_folder = \"/content/PROJECT_FOLDER\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(repo_folder, exist_ok=True)\n",
        "\n",
        "# List of files/folders to move (update as needed)\n",
        "items_to_move = [\n",
        "    \"gemini_batch_prompt_runner (1).ipynb\",\n",
        "    \"screenshots\",\n",
        "    \"output.csv\",\n",
        "    \"prompts.csv\",\n",
        "    \"sample_data\"\n",
        "]\n",
        "\n",
        "# Move each item into the project folder\n",
        "for item in items_to_move:\n",
        "    if os.path.exists(item):\n",
        "        shutil.move(item, os.path.join(repo_folder, item))\n",
        "\n",
        "# Confirm contents\n",
        "!ls {repo_folder}\n"
      ],
      "metadata": {
        "id": "MV-nyvGBLO7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 â€“ Batch Workflow Function\n",
        "\n",
        "This function reads prompts from a CSV, sends them to the Gemini API, and records responses with timestamps.\n",
        "\n",
        "Features:\n",
        "- Fully parameterized (model, temperature, max tokens, delay)\n",
        "- Retry logic for API errors\n",
        "- Outputs saved to a timestamped CSV\n",
        "- Compatible with batch processing of multiple prompts\n"
      ],
      "metadata": {
        "id": "9dGVTyjEYxPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 2: Batch Workflow Function with parameters and retry logic\n",
        "def run_batch_prompts(\n",
        "    input_csv,\n",
        "    output_dir=\".\",\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.7,\n",
        "    max_output_tokens=512,\n",
        "    delay=1,\n",
        "    retries=3\n",
        "):\n",
        "    \"\"\"\n",
        "    Reads prompts from CSV, sends them to Gemini, saves responses with timestamps.\n",
        "\n",
        "    Args:\n",
        "        input_csv (str): CSV file containing 'prompt' column.\n",
        "        output_dir (str): Directory to save output CSV.\n",
        "        model (str): Gemini model to use.\n",
        "        temperature (float): Model temperature.\n",
        "        max_output_tokens (int): Max tokens per response.\n",
        "        delay (float): Seconds to wait between API calls.\n",
        "        retries (int): Number of retries on API failure.\n",
        "    \"\"\"\n",
        "    prompts_df = pd.read_csv(input_csv)\n",
        "    responses = []\n",
        "    timestamps = []\n",
        "\n",
        "    total = len(prompts_df)\n",
        "    for idx, prompt in enumerate(tqdm(prompts_df['prompt'], desc=\"Generating responses\"), start=1):\n",
        "        attempt = 0\n",
        "        while attempt < retries:\n",
        "            try:\n",
        "                response = client.models.generate_content(\n",
        "                    model=model,\n",
        "                    contents=prompt,\n",
        "                    config=genai.types.GenerateContentConfig(\n",
        "                        temperature=temperature,\n",
        "                        max_output_tokens=max_output_tokens\n",
        "                    )\n",
        "                )\n",
        "                text = response.text\n",
        "                break  # Success\n",
        "            except Exception as e:\n",
        "                attempt += 1\n",
        "                text = f\"Error on attempt {attempt}: {e}\"\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "        responses.append(text)\n",
        "        timestamps.append(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "        time.sleep(delay)\n",
        "\n",
        "    prompts_df['response'] = responses\n",
        "    prompts_df['timestamp'] = timestamps\n",
        "\n",
        "    # Save output CSV with timestamp\n",
        "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_csv = os.path.join(output_dir, f\"output_{ts}.csv\")\n",
        "    prompts_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Batch complete. Responses saved to {output_csv}\")\n",
        "    return prompts_df\n"
      ],
      "metadata": {
        "id": "7gjbCqFXASjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "    print(m.name)\n"
      ],
      "metadata": {
        "id": "2aR9ctDWAdsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.genai import Client\n",
        "\n",
        "# Option 1: Set your API key in an environment variable called GENAI_API_KEY\n",
        "api_key = os.getenv(\"GENAI_API_KEY\")\n",
        "client = Client(api_key=api_key)\n",
        "\n",
        "# Option 2: Or define it once in a variable at the top of your notebook\n",
        "# GENAI_API_KEY = \"AIzaSyAtbxw812lmKIZh3uAS-CYkSJ3CT8RTf-c\"\n",
        "# client = Client(api_key=GENAI_API_KEY)\n"
      ],
      "metadata": {
        "id": "7iZ_GL0tLibd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.genai import types\n",
        "\n",
        "def send_prompt(prompt,\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                temperature=0.7,\n",
        "                max_output_tokens=256):\n",
        "    resp = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=temperature,\n",
        "            max_output_tokens=max_output_tokens\n",
        "        )\n",
        "    )\n",
        "    return resp.text\n",
        "\n",
        "print(send_prompt(\"Write a short summary of the impact of AI in healthcare.\"))\n"
      ],
      "metadata": {
        "id": "-58lee86AiWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"prompt\": [\n",
        "        \"Summarize the benefits of AI in healthcare.\",\n",
        "        \"Write a short motivational quote.\",\n",
        "        \"Explain machine learning to a beginner.\",\n",
        "        \"List 5 creative blog post ideas about fitness.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "df.to_csv(\"prompts.csv\", index=False)\n",
        "df\n"
      ],
      "metadata": {
        "id": "mQyYjWsSA6gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct way to list models\n",
        "for model in client.models.list():\n",
        "    print(model.name)"
      ],
      "metadata": {
        "id": "G5Tt1LegLx_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.genai import types\n",
        "\n",
        "# Load prompts\n",
        "df = pd.read_csv(\"prompts.csv\")\n",
        "\n",
        "# Prepare an empty list for responses\n",
        "responses = []\n",
        "\n",
        "# Process each row\n",
        "for i, row in df.iterrows():\n",
        "    prompt = row[\"prompt\"]\n",
        "\n",
        "    try:\n",
        "        resp = client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.7,\n",
        "                max_output_tokens=512\n",
        "            )\n",
        "        )\n",
        "        text = resp.text\n",
        "\n",
        "    except Exception as e:\n",
        "        text = f\"ERROR: {str(e)}\"\n",
        "\n",
        "    responses.append(text)\n",
        "\n",
        "# Add responses to DataFrame\n",
        "df[\"response\"] = responses\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"output.csv\", index=False)\n",
        "\n",
        "df  # Display the result\n"
      ],
      "metadata": {
        "id": "Hj6kQ8EVBFLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the output\n",
        "df_output = pd.read_csv(\"output.csv\")\n",
        "df_output.head(10)  # Shows the first 10 rows\n"
      ],
      "metadata": {
        "id": "wq5vjxS3Cbkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Bb9qXcm1Hf_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini Batch Prompt Runner â€“ Portfolio Project\n",
        "\n",
        "## Overview\n",
        "This notebook demonstrates a batch AI automation workflow using the Gemini LLM.  \n",
        "Prompts are stored in a CSV (`prompts.csv`), sent to Gemini via a robust batch function, and responses are saved in a timestamped CSV.\n",
        "\n",
        "## How It Works\n",
        "1. Load `prompts.csv` containing your prompts.\n",
        "2. Use **Section 2 â€“ Batch Workflow Function** to send prompts to Gemini.\n",
        "3. Responses are saved in a timestamped output CSV for review or further use.\n",
        "\n",
        "## Skills Demonstrated\n",
        "- Python scripting for automation\n",
        "- CSV data processing\n",
        "- Generative AI API integration\n",
        "- Error handling with retry logic\n",
        "- Batch processing and reproducible workflows\n",
        "- Workflow visualization using Graphviz\n",
        "\n",
        "## How to Run\n",
        "1. Ensure `prompts.csv` is in the same directory.\n",
        "2. Run **Section 1** to input your API key securely.\n",
        "3. Run **Section 2** to define the batch function.\n",
        "4. Run **Section 3** to execute the batch workflow and save outputs.\n"
      ],
      "metadata": {
        "id": "dQUbCtuUDHHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show google-genai\n"
      ],
      "metadata": {
        "id": "-EKtLt_ZMZKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame with example prompts\n",
        "prompts_df = pd.DataFrame({\n",
        "    'prompt': [\n",
        "        \"Explain reinforcement learning in simple terms.\",\n",
        "        \"Give me a Python code snippet for a for-loop.\",\n",
        "        \"Summarize the benefits of using AI in healthcare.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Save to CSV in the notebook environment\n",
        "prompts_df.to_csv('prompts.csv', index=False)\n",
        "\n",
        "# Verify it\n",
        "print(prompts_df)\n"
      ],
      "metadata": {
        "id": "KD8-_EcmZBzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.genai import Client\n",
        "\n",
        "client = Client(api_key=os.getenv(\"GENAI_API_KEY\"))\n"
      ],
      "metadata": {
        "id": "RJdCpoHQZMl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses = []\n",
        "\n",
        "for prompt in prompts_df['prompt']:\n",
        "    try:\n",
        "        response = client.generate_text(model=\"gemini-2.0-flash\", prompt=prompt)\n",
        "        responses.append(response.text)\n",
        "    except Exception as e:\n",
        "        responses.append(f\"Error: {e}\")\n",
        "\n",
        "# Add responses to DataFrame\n",
        "prompts_df['response'] = responses\n",
        "\n",
        "# Save to CSV\n",
        "prompts_df.to_csv('output.csv', index=False)\n",
        "\n",
        "# Preview results\n",
        "print(prompts_df)\n"
      ],
      "metadata": {
        "id": "77rZC5nIZd-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses = []\n",
        "\n",
        "for prompt in prompts_df['prompt']:\n",
        "    try:\n",
        "        response = client.responses.create(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            prompt=prompt\n",
        "        )\n",
        "        responses.append(response.output_text)  # Extract the text\n",
        "    except Exception as e:\n",
        "        responses.append(f\"Error: {e}\")\n",
        "\n",
        "# Add responses to DataFrame\n",
        "prompts_df['response'] = responses\n",
        "\n",
        "# Save to CSV\n",
        "prompts_df.to_csv('output.csv', index=False)\n",
        "\n",
        "# Preview results\n",
        "print(prompts_df)\n"
      ],
      "metadata": {
        "id": "kFzFGYEUZkHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "responses = []\n",
        "for prompt in tqdm(prompts_df['prompt'], desc=\"Generating responses\"):\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash\",\n",
        "            contents=prompt\n",
        "        )\n",
        "        responses.append(response.text)\n",
        "    except Exception as e:\n",
        "        responses.append(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "3BmB21FXZpGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google import genai\n",
        "import os\n",
        "\n",
        "def run_batch_prompts(input_csv, output_csv, api_key=None):\n",
        "    # Use API key from argument or environment variable\n",
        "    if api_key is None:\n",
        "        api_key = os.getenv(\"GENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key must be provided either as an argument or via GENAI_API_KEY environment variable.\")\n",
        "\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    prompts_df = pd.read_csv(input_csv)\n",
        "    responses = []\n",
        "\n",
        "    for prompt in prompts_df['prompt']:\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=\"gemini-2.0-flash\",\n",
        "                contents=prompt\n",
        "            )\n",
        "            responses.append(response.text)\n",
        "        except Exception as e:\n",
        "            responses.append(f\"Error: {e}\")\n",
        "\n",
        "    prompts_df['response'] = responses\n",
        "    prompts_df.to_csv(output_csv, index=False)\n",
        "    print(prompts_df)\n",
        "\n",
        "# Example usage:\n",
        "# Set environment variable GENAI_API_KEY instead of hardcoding\n",
        "# run_batch_prompts('prompts.csv', 'output.csv')\n"
      ],
      "metadata": {
        "id": "_tOj1mXXZ7TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 3 â€“ Run Batch Process\n",
        "\n",
        "input_csv = \"prompts.csv\"\n",
        "output_csv = \"output.csv\"  # or include a timestamp if desired\n",
        "\n",
        "# Run batch function using environment API key\n",
        "run_batch_prompts(input_csv=input_csv, output_csv=output_csv)\n",
        "\n",
        "# Load the CSV to preview results\n",
        "import pandas as pd\n",
        "df_results = pd.read_csv(output_csv)\n",
        "df_results.head()\n"
      ],
      "metadata": {
        "id": "YIgjAA3paPts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google import genai\n",
        "import time\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def run_batch_prompts_pro(input_csv, output_csv, model=\"gemini-2.0-flash\", delay=1):\n",
        "    \"\"\"\n",
        "    Reads prompts from input_csv, sends them to Gemini,\n",
        "    saves responses to output_csv with timestamps and optional delay.\n",
        "\n",
        "    Args:\n",
        "        input_csv (str): Path to CSV containing a 'prompt' column.\n",
        "        output_csv (str): Path to save results.\n",
        "        model (str): Gemini model to use (default: gemini-2.0-flash).\n",
        "        delay (int | float): Seconds to wait between API calls (default: 1).\n",
        "    \"\"\"\n",
        "    # Use API key from environment variable\n",
        "    api_key = os.getenv(\"GENAI_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Set GENAI_API_KEY in your environment.\")\n",
        "\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    prompts_df = pd.read_csv(input_csv)\n",
        "    responses = []\n",
        "    timestamps = []\n",
        "\n",
        "    total = len(prompts_df)\n",
        "    for idx, prompt in enumerate(prompts_df['prompt'], start=1):\n",
        "        print(f\"Processing {idx}/{total}...\")\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=model,\n",
        "                contents=prompt\n",
        "            )\n",
        "            responses.append(response.text)\n",
        "        except Exception as e:\n",
        "            responses.append(f\"Error: {e}\")\n",
        "        timestamps.append(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "        time.sleep(delay)\n",
        "\n",
        "    # Add results to DataFrame\n",
        "    prompts_df['response'] = responses\n",
        "    prompts_df['timestamp'] = timestamps\n",
        "\n",
        "    # Save to CSV\n",
        "    prompts_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Batch complete. Responses saved to {output_csv}\")\n",
        "    print(prompts_df)\n",
        "\n",
        "# Example usage:\n",
        "# Make sure to set GENAI_API_KEY in your environment first\n",
        "# run_batch_prompts_pro('prompts.csv', 'output.csv', delay=1)\n"
      ],
      "metadata": {
        "id": "knJgAEYfaccI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Prepare Prompts\n",
        "Create a CSV with a column named `prompt`. This CSV will be used to batch-generate responses from Gemini.\n"
      ],
      "metadata": {
        "id": "scaASomSbTJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "prompts_df = pd.DataFrame({\n",
        "    'prompt': [\n",
        "        \"Explain reinforcement learning in simple terms.\",\n",
        "        \"Give me a Python code snippet for a for-loop.\",\n",
        "        \"Summarize the benefits of using AI in healthcare.\"\n",
        "    ]\n",
        "})\n",
        "prompts_df.to_csv('prompts.csv', index=False)\n",
        "print(prompts_df)\n"
      ],
      "metadata": {
        "id": "sfVJzAEXbVS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/drive/MyDrive/AI_Portfolio/screenshots\n",
        "!cp -r screenshots/* /content/drive/MyDrive/AI_Portfolio/screenshots/\n"
      ],
      "metadata": {
        "id": "zDvxO58tcvFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "screenshots = [\n",
        "    (\"Section1.1.png\", \"Figure 1: Step 1 â€“ Prepare Prompts\"),\n",
        "    (\"Section1.2.png\", \"Figure 2: Step 1 â€“ Preview CSV\"),\n",
        "\n",
        "]\n",
        "\n",
        "for file, caption in screenshots:\n",
        "    display(Markdown(f\"**{caption}**\"))\n",
        "    display(Image(filename=f'screenshots/{file}'))\n"
      ],
      "metadata": {
        "id": "QADGAYs6kqG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Batch Workflow Function\n",
        "This function reads prompts from CSV, sends them to Gemini, records timestamps, and saves output CSV.\n"
      ],
      "metadata": {
        "id": "0EDsGM0bbXvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the output CSV\n",
        "import pandas as pd\n",
        "\n",
        "output_df = pd.read_csv('output.csv')\n",
        "print(output_df)\n"
      ],
      "metadata": {
        "id": "u5ebYmM6bZ-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Figure 2: Step 2 â€“ Define Batch Workflow Function**\n",
        "This screenshot shows the batch function that reads prompts, sends them to Gemini, and saves the responses.\n"
      ],
      "metadata": {
        "id": "_yKuyC_VlqP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "screenshots = [\n",
        "\n",
        "    (\"Section2.1.png\", \"Figure 3: Step 2 â€“ Batch Function\"),\n",
        "    (\"Section2.2.png\", \"Figure 4: Step 2 â€“ Function Preview\"),\n",
        "    (\"Section2.3.png\", \"Figure 5: Step 2 â€“ Error Handling\"),\n",
        "    (\"Section2.4.png\", \"Figure 6: Step 2 â€“ Timestamping\"),\n",
        "    (\"Section2.5.png\", \"Figure 7: Step 2 â€“ Output Saved\"),\n",
        "\n",
        "]\n",
        "\n",
        "for file, caption in screenshots:\n",
        "    display(Markdown(f\"**{caption}**\"))\n",
        "    display(Image(filename=f'screenshots/{file}'))\n"
      ],
      "metadata": {
        "id": "7FptOrNelrOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Run Batch Process\n",
        "Call the batch workflow function to send prompts from `prompts.csv` to Gemini,\n",
        "capture the responses, and save them to `output.csv`.  \n",
        "You can adjust the model or add a delay between requests if needed.\n"
      ],
      "metadata": {
        "id": "JRVR7WYRbtna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your GENAI_API_KEY in your environment before running the notebook\n",
        "# Example (Linux/Mac):\n",
        "# export GENAI_API_KEY=\"YOUR_API_KEY_HERE\"\n",
        "# Example (Windows):\n",
        "# set GENAI_API_KEY=\"YOUR_API_KEY_HERE\"\n"
      ],
      "metadata": {
        "id": "nl9a8ScjbvPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure GENAI_API_KEY is set in your environment\n",
        "# For example, in a notebook you can do:\n",
        "import os\n",
        "os.environ[\"GENAI_API_KEY\"] = \"AIzaSyAtbxw812lmKIZh3uAS-CYkSJ3CT8RTf-c\"\n",
        "\n",
        "# Then call the function without the API key argument\n",
        "run_batch_prompts('prompts.csv', 'output.csv')\n"
      ],
      "metadata": {
        "id": "MvbuMwcgbxk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "screenshots = [\n",
        "\n",
        "    (\"Section3.1.png\", \"Figure 8: Step 3 â€“ Run Batch Process and Output Preview\"),\n",
        "\n",
        "]\n",
        "\n",
        "for file, caption in screenshots:\n",
        "    display(Markdown(f\"**{caption}**\"))\n",
        "    display(Image(filename=f'screenshots/{file}'))"
      ],
      "metadata": {
        "id": "AZ2lZ2_dZNu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4 â€“ Workflow Diagram\n",
        "The diagram below shows the end-to-end process of the batch prompt runner:\n",
        "\n",
        "1. Load prompts from `prompts.csv`\n",
        "2. Use **Section 2 â€“ Batch Workflow Function** to send prompts to Gemini\n",
        "3. Collect responses\n",
        "4. Save responses to a timestamped output CSV\n"
      ],
      "metadata": {
        "id": "A3RJfeAic5RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Graphviz if not already installed\n",
        "!pip install graphviz\n",
        "\n",
        "from graphviz import Digraph\n",
        "\n",
        "# Create workflow diagram\n",
        "dot = Digraph(comment='Batch Prompt Workflow')\n",
        "\n",
        "dot.node('A', 'prompts.csv')\n",
        "dot.node('B', 'Section 2 â€“ Batch Workflow Function')\n",
        "dot.node('C', 'Gemini API')\n",
        "dot.node('D', 'output_TIMESTAMP.csv')\n",
        "\n",
        "dot.edges(['AB', 'BC', 'CD'])\n",
        "\n",
        "# Render and view diagram in Colab\n",
        "dot.render('workflow.gv', view=True)\n",
        "dot\n"
      ],
      "metadata": {
        "id": "5vwR3MU6cbFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display, Markdown\n",
        "\n",
        "screenshots = [\n",
        "    (\"Section4.1.png\", \"Figure 1: Section 4 â€“ Workflow Diagram\"),\n",
        "\n",
        "]\n",
        "\n",
        "for file, caption in screenshots:\n",
        "    display(Markdown(f\"**{caption}**\"))\n",
        "    display(Image(filename=f'screenshots/{file}'))"
      ],
      "metadata": {
        "id": "G_h587KnSJOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skills Demonstrated & Next Steps\n",
        "\n",
        "### Skills Demonstrated\n",
        "- **Python Scripting & Automation**: Developed reusable batch functions for processing multiple prompts.\n",
        "- **Generative AI Integration**: Used the Google Gemini API to generate text responses programmatically.\n",
        "- **Data Handling with CSV**: Loaded prompts from CSV, processed responses, and saved results cleanly.\n",
        "- **Error Handling & Logging**: Implemented try/except blocks to handle API errors and record timestamps.\n",
        "- **Workflow Visualization**: Created workflow diagrams to illustrate the end-to-end batch processing pipeline.\n",
        "- **Version Control Readiness**: Structured the notebook and folder layout for reproducibility and GitHub portfolio inclusion.\n",
        "- **Notebook Best Practices**: Secure API key handling, clean display of outputs, and clear stepwise documentation.\n",
        "\n",
        "### Next Steps\n",
        "- **Enhance Batch Workflow**: Add support for multiple Gemini models, prompt versioning, or adjustable parameters like temperature and max tokens.\n",
        "- **Interactive UI**: Develop a simple interface to upload prompts, select models, and visualize responses without editing code cells.\n",
        "- **Expanded Use Cases**: Experiment with different prompt types, embeddings, or multi-turn conversations.\n",
        "- **Portfolio Refinement**: Fine-tune screenshot captions, add more visual evidence, and ensure the notebook is polished for presentation.\n",
        "- **Deployment**: Package the workflow into a script or web app for more accessible usage outside Colab.\n",
        "\n",
        "> This notebook demonstrates a repeatable, end-to-end workflow for batch-generating AI responses, combining automation, data handling, and visual documentation in a portfolio-ready format.\n"
      ],
      "metadata": {
        "id": "b13c-ExLb0sh"
      }
    }
  ]
}